{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install glmnet_py\n",
    "#!pip install glmnet\n",
    "#!pip show glmnet_py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip show scipy\n",
    "#!pip install scipy==1.11.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "if os.path.exists('./glmnet/GLMnet.so'):\n",
    "    os.remove('./glmnet/GLMnet.so')\n",
    "os.system('gfortran ./glmnet/GLMnet.f -fPIC -fdefault-real-8 -shared -o ./glmnet/GLMnet.so')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scipy, importlib, pprint, matplotlib.pyplot as plt, warnings\n",
    "from scipy import stats\n",
    "from glmnet import glmnet; from glmnetPlot import glmnetPlot \n",
    "from glmnetPrint import glmnetPrint; from glmnetCoef import glmnetCoef; from glmnetPredict import glmnetPredict\n",
    "from cvglmnet import cvglmnet; from cvglmnetCoef import cvglmnetCoef\n",
    "from cvglmnetPlot import cvglmnetPlot; from cvglmnetPredict import cvglmnetPredict\n",
    "\n",
    "from aml_utils import test_case_checker, perform_computation\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Linear regression with various regularizers** The UCI Machine Learning dataset repository hosts a dataset giving features of music, and the location (latitude and longitude) at which that music originate. There are actually two versions of this dataset. Either one is OK, but I think you'll find the one with more independent variables more interesting. In this assignment you will investigate methods to predict music location from the provided features. You should regard latitude and longitude as entirely independent.\n",
    "  * First, build a straightforward linear regression of location (latitude and longitude) against features. What is the R-squared? Plot a graph evaluating each regression.\n",
    "  * Does a Box-Cox transformation improve the regressions? Notice that the dependent variable has some negative values, which Box-Cox doesn't like. You can deal with this by remembering that these are angles, so you get to choose the origin. For the rest of the exercise, use the transformation if it does improve things, otherwise, use the raw data.\n",
    "  * Use glmnet to produce:\n",
    "    * A regression regularized by L2 (a ridge regression). You should estimate the regularization coefficient that produces the minimum error. Is the regularized regression better than the unregularized regression?\n",
    "    * A regression regularized by L1 (a lasso regression). You should estimate the regularization coefficient that produces the minimum error. How many variables are used by this regression? Is the regularized regression better than the unregularized regression?\n",
    "    * A regression regularized by elastic net (equivalently, a regression regularized by a convex combination of L1 and L2 weighted by a parameter `alpha`). Try three values of `alpha`. You should estimate the regularization coefficient `lambda` that produces the minimum error. How many variables are used by this regression? Is the regularized regression better than the unregularized regression?\n",
    "2. **Logistic regression** The UCI Machine Learning dataset repository hosts a dataset giving whether a Taiwanese credit card user defaults against a variety of features here. In this part of the assignment you will use logistic regression to predict whether the user defaults. You should ignore outliers, but you should try the various regularization schemes discussed above.\n",
    "\n",
    "<font color='red'>Attention:</font> After finishing this notebook, you will need to do a follow-up quiz as well. The overall grade for this assignment is based on this notebook and the follow-up quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Machine Learning dataset repository hosts a dataset that provides a set of features of music, and the location (latitude and longitude) at which that music originates at https://archive.ics.uci.edu/ml/datasets/Geographical+Original+of+Music. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Input/Output**: This data has 118 columns; the first 116 columns are the music features, and the last two columns are the music origin's latitude and the longitude, respectively.\n",
    "\n",
    "* **Missing Data**: There is no missing data.\n",
    "\n",
    "* **Final Goal**: We want to **properly** fit a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('music/default_plus_chromatic_features_1059_tracks.txt', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = df.iloc[:,:-2].values\n",
    "lat_full = df.iloc[:,-2].values\n",
    "lon_full = df.iloc[:,-1].values\n",
    "X_full.shape, lat_full.shape, lon_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Dependent Variables Positive\n",
    "\n",
    "This will make the data compatible with the box-cox transformation that we will later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_full = 90 + lat_full\n",
    "lon_full = 180 + lon_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector = 'LOF'\n",
    "\n",
    "if outlier_detector == 'LOF':\n",
    "    outlier_clf = LocalOutlierFactor(novelty=False)\n",
    "elif outlier_detector == 'IF':\n",
    "    outlier_clf = IsolationForest(warm_start=True, random_state=12345)\n",
    "elif outlier_detector == 'EE':\n",
    "    outlier_clf = EllipticEnvelope(random_state=12345)\n",
    "else:\n",
    "    outlier_clf = None\n",
    "\n",
    "is_not_outlier = outlier_clf.fit_predict(X_full) if outlier_clf is not None else np.ones_like(lat_full)>0\n",
    "X_useful = X_full[is_not_outlier==1,:]\n",
    "lat_useful = lat_full[is_not_outlier==1]\n",
    "lon_useful = lon_full[is_not_outlier==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion**: You may find it instructive to explore the effect of the different outlier detection methods on the accuracy of the linear regression model. \n",
    "\n",
    "There is a brief introduction about each of the implemented OD methods along with some nice visualizations at https://scikit-learn.org/stable/modules/outlier_detection.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_indices, test_indices = train_test_split(np.arange(X_useful.shape[0]), test_size=0.2, random_state=12345)\n",
    "\n",
    "X_train_val = X_useful[train_val_indices, :]\n",
    "lat_train_val = lat_useful[train_val_indices]\n",
    "lon_train_val = lon_useful[train_val_indices]\n",
    "\n",
    "X_test = X_useful[test_indices, :]\n",
    "lat_test = lat_useful[test_indices]\n",
    "lon_test = lon_useful[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Building a Simple Linear Regression Model (Scikit-Learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "if perform_computation:\n",
    "    X, Y = X_train_val, lat_train_val\n",
    "    reg_lat = LinearRegression().fit(X, Y)\n",
    "    train_r2_lat = reg_lat.score(X,Y)\n",
    "    fitted_lat = reg_lat.predict(X)\n",
    "    residuals_lat = Y-fitted_lat\n",
    "    train_mse_lat = (residuals_lat**2).mean()\n",
    "    test_mse_lat = np.mean((reg_lat.predict(X_test)-lat_test)**2)\n",
    "    test_r2_lat = reg_lat.score(X_test,lat_test)\n",
    "\n",
    "    X, Y = X_train_val, lon_train_val\n",
    "    reg_lon = LinearRegression().fit(X, Y)\n",
    "    train_r2_lon = reg_lon.score(X,Y)\n",
    "    fitted_lon = reg_lon.predict(X)\n",
    "    residuals_lon = Y-fitted_lon\n",
    "    train_mse_lon = (residuals_lon**2).mean()\n",
    "    test_mse_lon = np.mean((reg_lon.predict(X_test)-lon_test)**2)\n",
    "    test_r2_lon = reg_lon.score(X_test,lon_test)\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,6.), dpi=100)\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.scatter(fitted_lat, residuals_lat)\n",
    "    ax.set_xlabel('Fitted Latitude')\n",
    "    ax.set_ylabel('Latitude Residuals')\n",
    "    _ = ax.set_title(f'Residuals Vs. Fitted Latitude.\\n' +\n",
    "                     f'Training R2=%.3f, Testing R2=%.3f\\n' % (train_r2_lat, test_r2_lat) +\n",
    "                     f'Training MSE=%.3f, Testing MSE=%.3f' % (train_mse_lat, test_mse_lat))\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.scatter(fitted_lon, residuals_lon)\n",
    "    ax.set_xlabel('Fitted Longitude')\n",
    "    ax.set_ylabel('Longitude Residuals')\n",
    "    _ = ax.set_title(f'Residuals Vs. Fitted Longitude.\\n' +\n",
    "                     f'Training R2=%.3f, Testing R2=%.3f\\n' % (train_r2_lon, test_r2_lon) +\n",
    "                     f'Training MSE=%.3f, Testing MSE=%.3f' % (train_mse_lon, test_mse_lon))\n",
    "    fig.set_tight_layout([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Building a Simple Linear Regression (glmnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `glmnet_vanilla` that fits a linear regression model from the glmnet library, and takes the following arguments as input:\n",
    "\n",
    "1. `X_train`: A numpy array of the shape `(N,d)` where `N` is the number of training data points, and `d` is the data dimension. Do not assume anything about `N` or `d` other than being a positive integer.\n",
    "2. `Y_train`: A numpy array of the shape `(N,)` where `N` is the number of training data points.\n",
    "3. `X_test`: A numpy array of the shape `(N_test,d)` where `N_test` is the number of testing data points, and `d` is the data dimension.\n",
    "\n",
    "Your model should train on the training features and labels, and then predict on the test data. Your model should return the following two items:\n",
    "\n",
    "1. `fitted_Y`: The predicted values on the test data as a numpy array with a shape of `(N_test,)` where `N_test` is the number of testing data points.\n",
    "\n",
    "2. `glmnet_model`: The glmnet library's returned model stored as a python dictionary.\n",
    "\n",
    "**Important Notes**:\n",
    "1. **Do not** play with the default options unless you're instructed to.\n",
    "2. You may find this glmnet documentation helpful: https://github.com/bbalasub1/glmnet_python/blob/master/test/glmnet_examples.ipynb\n",
    "    * You may find it useful to read about the gaussian family in the first section, the functions `glmnet` and `glmnetPredict`, and their arguments.\n",
    "3. **Do not** perform any cross-validation for this task.\n",
    "4. **Do not** play with the regularization settings in the **training call**.\n",
    "5. **For prediction** on the test data, make sure that a **regularization coefficient of 0** was used. \n",
    "6. You may need to choose the proper `family` variable when you're training the model.\n",
    "7. You may need to choose the proper `ptype` variable when you're predicting on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c9eb89fd8a59964aca8f20787293b07",
     "grade": false,
     "grade_id": "cell-6785a420e10621a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glmnet_vanilla(X_train, Y_train, X_test=None):\n",
    "    if X_test is None:\n",
    "        X_test = X_train.copy().astype(np.float64)\n",
    "    # Creating Scratch Variables For glmnet Consumption\n",
    "    X_train = X_train.copy().astype(np.float64)\n",
    "    Y_train = Y_train.copy().astype(np.float64)\n",
    "    ##Por lo que leí en la documentación de glmnet, por linear se refieren a gaussian\n",
    "    \n",
    "    glmnet_model = glmnet(x=X_train, y=Y_train, family='gaussian')\n",
    "    \n",
    "\n",
    "    fitted_Y = glmnetPredict(glmnet_model, X_test, s=scipy.float64([0.00]), ptype=\"response\").flatten()\n",
    "    \n",
    "    assert fitted_Y.shape == (X_test.shape[0],), 'fitted_Y should not be two dimensional (hint: reshaping may be helpful)'\n",
    "    assert isinstance(glmnet_model, dict)\n",
    "    assert list(glmnet_model.keys()) == ['a0','beta','dev','nulldev','df','lambdau','npasses','jerr','dim','offset','class']\n",
    "    return fitted_Y, glmnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86762e1e2651868289da6cf62186b32c",
     "grade": true,
     "grade_id": "cell-d5a4943d1ed88252",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(35).reshape(7,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "some_pred, some_model = glmnet_vanilla(some_X, some_Y)\n",
    "assert np.array_equal(some_pred.round(3), np.array([20.352, 44.312, 39.637, 74.146, 20.352, 49.605, 24.596]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(lambda *args,**kwargs: glmnet_vanilla(*args,**kwargs)[0], task_id=1)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(trainer):\n",
    "    # Latitude Training, Prediction, Evaluation, etc.\n",
    "    lat_pred_train = trainer(X_train_val, lat_train_val, X_train_val)[0]\n",
    "    train_r2_lat = r2_score(lat_train_val, lat_pred_train)\n",
    "    residuals_lat = lat_train_val - lat_pred_train\n",
    "    train_mse_lat = (residuals_lat**2).mean()\n",
    "    lat_pred_test = trainer(X_train_val, lat_train_val, X_test)[0]\n",
    "    test_mse_lat = np.mean((lat_pred_test-lat_test)**2)\n",
    "    test_r2_lat = r2_score(lat_test, lat_pred_test)\n",
    "\n",
    "    # Longitude Training, Prediction, Evaluation, etc.\n",
    "    lon_pred_train = trainer(X_train_val, lon_train_val, X_train_val)[0]\n",
    "    train_r2_lon = r2_score(lon_train_val, lon_pred_train)\n",
    "    residuals_lon = lon_train_val - lon_pred_train\n",
    "    train_mse_lon = (residuals_lon**2).mean()\n",
    "    lon_pred_test = trainer(X_train_val, lon_train_val, X_test)[0]\n",
    "    test_mse_lon = np.mean((lon_pred_test-lon_test)**2)\n",
    "    test_r2_lon = r2_score(lon_test, lon_pred_test)\n",
    "\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,6.), dpi=100)\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.scatter(lat_pred_train, residuals_lat)\n",
    "    ax.set_xlabel('Fitted Latitude')\n",
    "    ax.set_ylabel('Latitude Residuals')\n",
    "    _ = ax.set_title(f'Residuals Vs. Fitted Latitude.\\n' +\n",
    "                     f'Training R2=%.3f, Testing R2=%.3f\\n' % (train_r2_lat, test_r2_lat) +\n",
    "                     f'Training MSE=%.3f, Testing MSE=%.3f' % (train_mse_lat, test_mse_lat))\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.scatter(lon_pred_train, residuals_lon)\n",
    "    ax.set_xlabel('Fitted Longitude')\n",
    "    ax.set_ylabel('Longitude Residuals')\n",
    "    _ = ax.set_title(f'Residuals Vs. Fitted Longitude.\\n' +\n",
    "                     f'Training R2=%.3f, Testing R2=%.3f\\n' % (train_r2_lon, test_r2_lon) +\n",
    "                     f'Training MSE=%.3f, Testing MSE=%.3f' % (train_mse_lon, test_mse_lon))\n",
    "    fig.set_tight_layout([0, 0, 1, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    train_and_plot(glmnet_vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Box-Cox Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `boxcox_lambda` that takes a numpy array `y` as input, and produce the best box-cox transformation $\\lambda$ parameter `best_lam` as a scalar. \n",
    "\n",
    "**Hint**: Do not implement this function yourself. You may find some useful function here https://docs.scipy.org/doc/scipy/reference/stats.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd7a7e0db2b3d850c189de6027e9085f",
     "grade": false,
     "grade_id": "cell-b890cc74d0a27f33",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def boxcox_lambda(y):\n",
    "    assert y.ndim==1\n",
    "    assert (y>0).all()\n",
    "    \n",
    "    _, best_lam = scipy.stats.boxcox(y)\n",
    "\n",
    "    \n",
    "    return best_lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23724e1e8146373413fefd96b6f537ad",
     "grade": true,
     "grade_id": "cell-2040f71f314d3598",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(35).reshape(7,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "assert boxcox_lambda(some_Y).round(3) == -0.216\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(boxcox_lambda, task_id=2)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `boxcox_transform` that takes a numpy array `y` and the box-cox transformation $\\lambda$ parameter `lam` as input, and returns the numpy array `transformed_y` which is the box-cox transformation of `y` using $\\lambda$. \n",
    "\n",
    "**Hint**: Do not implement this function yourself. You may find some useful function here https://docs.scipy.org/doc/scipy/reference/stats.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1423295e95b9198d1a90c8cc70c45a7c",
     "grade": false,
     "grade_id": "cell-362af67d00cb7923",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def boxcox_transform(y, lam):\n",
    "    assert y.ndim==1\n",
    "    assert (y>0).all()\n",
    "    \n",
    "    transformed_y = scipy.stats.boxcox(y, lam)\n",
    "    \n",
    "    return transformed_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eda10bcc2f0a61348e18cf10f2f6b87b",
     "grade": true,
     "grade_id": "cell-b8bffc82a9388e13",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(35).reshape(7,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "assert np.array_equal(boxcox_transform(some_Y, lam=0).round(3), np.array([2.996, 3.807, 3.689, 4.317, 2.996, 3.892, 3.178]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(boxcox_transform, task_id=3)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `boxcox_inv_transform` that takes a numpy array `transformed_y` and the box-cox transformation $\\lambda$ parameter `lam` as input, and returns the numpy array `y` which is the inverse box-cox transformation of `transformed_y` using $\\lambda$. \n",
    "\n",
    "1. If $\\lambda \\neq 0$: \n",
    "$$y = |y^{bc}\\cdot \\lambda + 1|^{\\frac{1}{\\lambda}}$$\n",
    "2. If $\\lambda = 0$:\n",
    "$$y = e^{y^{bc}}$$\n",
    "\n",
    "**Hint**: You need to implement this function yourself!\n",
    "\n",
    "**Important Note**: Be very careful about the signs, absolute values, and raising to exponents with decimal points. For something to be raised to any power that is not a full integer, you need to make sure that the base is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcox_inv_transform(transformed_y, lam):\n",
    "\n",
    "    # Convert input to numpy array if it isn't already\n",
    "    transformed_y = np.asarray(transformed_y)\n",
    "    \n",
    "    if lam == 0:\n",
    "        # For λ = 0, simply exponentiate\n",
    "        y = np.exp(transformed_y)\n",
    "    else:\n",
    "        # For λ ≠ 0:\n",
    "        # First compute y^(bc)·λ + 1\n",
    "        inner_term = transformed_y * lam + 1\n",
    "        \n",
    "        # Take absolute value before raising to 1/λ power\n",
    "        # to ensure we don't try to take roots of negative numbers\n",
    "        y = np.abs(inner_term) ** (1.0/lam)\n",
    "        \n",
    "\n",
    "    \n",
    "    assert not np.isnan(y).any()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27e39ee472f7b157ecaa529ff3bc2a56",
     "grade": true,
     "grade_id": "cell-40dd0d43a2d5aef6",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(35).reshape(7,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)/10\n",
    "some_invbc = boxcox_inv_transform(some_Y, lam=0).round(3)\n",
    "assert np.array_equal(some_invbc, np.array([7.389, 90.017, 54.598, 1808.042, 7.389,  134.29 ,11.023]))\n",
    "\n",
    "another_invbc = boxcox_inv_transform(some_Y, lam=5).round(3)\n",
    "assert np.array_equal(another_invbc, np.array([1.615, 1.88 , 1.838, 2.075, 1.615, 1.911, 1.67 ]))\n",
    "\n",
    "iden = boxcox_inv_transform(boxcox_transform(some_Y, lam=5), lam=5).round(3)\n",
    "assert np.array_equal(iden, some_Y.round(3))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(boxcox_inv_transform, task_id=4)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 5</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the box-cox functions you previously wrote, write a function `glmnet_bc` that fits a linear regression model from the glmnet library with the box-cox transformation applied on the labels, and takes the following arguments as input:\n",
    "\n",
    "1. `X_train`: A numpy array of the shape `(N,d)` where `N` is the number of training data points, and `d` is the data dimension. Do not assume anything about `N` or `d` other than being a positive integer.\n",
    "2. `Y_train`: A numpy array of the shape `(N,)` where `N` is the number of training data points.\n",
    "3. `X_test`: A numpy array of the shape `(N_test,d)` where `N_test` is the number of testing data points, and `d` is the data dimension.\n",
    "\n",
    "Your model should train on the training features and labels, and then predict on the test data. Your model should return the following two items:\n",
    "\n",
    "1. `fitted_test`: The predicted values on the test data as a numpy array with a shape of `(N_test,)` where `N_test` is the number of testing data points.\n",
    "\n",
    "2. `glmnet_model`: The glmnet library's returned model stored as a python dictionary.\n",
    "\n",
    "You should first obtain the best box-cox lambda parameter from the training data. Then transform the training labels before passing them to the training procedure. This will cause the trained model to be operating on the box-cox transformed space. Therefore, the test predictions should be box-cox inverse transformed before reporting them as output. \n",
    "\n",
    "Use the `glmnet_vanilla` function you already written on the box-cox transformed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b66220358671047bf6c6fb6c2a622f83",
     "grade": false,
     "grade_id": "cell-885112d310d72cd8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glmnet_bc(X_train, Y_train, X_test=None):\n",
    "    \n",
    "    if X_test is None:\n",
    "        X_test = X_train.copy().astype(np.float64)\n",
    "\n",
    "    best_lambda = boxcox_lambda(Y_train)\n",
    "    Y_train_transformed = boxcox_transform(Y_train, best_lambda)\n",
    "    fitted_transformed, glmnet_model = glmnet_vanilla(X_train, Y_train_transformed, X_test)\n",
    "    fitted_test = boxcox_inv_transform(fitted_transformed, best_lambda)\n",
    "    \n",
    "    assert isinstance(glmnet_model, dict)\n",
    "    return fitted_test, glmnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea89e41ec0a83cfa6a7f8729dc80bc57",
     "grade": true,
     "grade_id": "cell-f2455339942351cd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(35).reshape(7,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "some_pred, some_model = glmnet_bc(some_X, some_Y)\n",
    "assert np.array_equal(some_pred.round(3), np.array([20.012, 42.985, 40.189, 75.252, 20.012, 50.095, 24.32 ]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(lambda *args,**kwargs: glmnet_bc(*args,**kwargs)[0], task_id=5)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    train_and_plot(glmnet_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 6</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `glmnet_ridge` that fits a Ridge-regression model from the glmnet library, and takes the following arguments as input:\n",
    "\n",
    "1. `X_train`: A numpy array of the shape `(N,d)` where `N` is the number of training data points, and `d` is the data dimension. Do not assume anything about `N` or `d` other than being a positive integer.\n",
    "2. `Y_train`: A numpy array of the shape `(N,)` where `N` is the number of training data points.\n",
    "3. `X_test`: A numpy array of the shape `(N_test,d)` where `N_test` is the number of testing data points, and `d` is the data dimension.\n",
    "\n",
    "Your model should train on the training features and labels, and then predict on the test data. Your model should return the following two items:\n",
    "\n",
    "1. `fitted_Y_test`: The predicted values on the test data as a numpy array with a shape of `(N_test,)` where `N_test` is the number of testing data points.\n",
    "\n",
    "2. `glmnet_model`: The glmnet library's returned model stored as a python dictionary.\n",
    "\n",
    "**Important Notes**:\n",
    "1. **Do not** play with the default options unless you're instructed to.\n",
    "2. You may find this glmnet documentation helpful: https://github.com/bbalasub1/glmnet_python/blob/master/test/glmnet_examples.ipynb\n",
    "  * You may find it useful to read about the gaussian family in the first section, cross-validation, the functions `cvglmnet` and `cvglmnetPredict`, and their arguments.\n",
    "3. You **should** perform **cross-validation** for this task.\n",
    "4. Use **10-folds** for cross-validation.\n",
    "5. Ask glmnet to search over **100** different values of the regularization coefficient.\n",
    "6. Use the **Mean Squared Error** as a metric for cross-validation.\n",
    "7. For **prediction**, use the **regularization coefficient** that produces the **minimum cross-validation MSE**.\n",
    "7. You may need to choose the proper `family` variable when you're training the model.\n",
    "8. You may need to choose the proper `ptype` variable when you're predicting on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ddd03c3ccba99ab173a63d3912915ef",
     "grade": false,
     "grade_id": "cell-eb4986f258867396",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glmnet_ridge(X_train, Y_train, X_test=None):\n",
    "    if X_test is None:\n",
    "        X_test = X_train.copy().astype(np.float64)\n",
    "    X_train = X_train.copy().astype(np.float64)\n",
    "    Y_train = Y_train.copy().astype(np.float64)\n",
    "    \n",
    "    glmnet_model = cvglmnet(x=X_train,y=Y_train, family='gaussian',alpha=0, folds=10,nlambda=100, ptype='mse')\n",
    "    ##Alpha = 0 para ridge\n",
    "    fitted_Y_test = cvglmnetPredict(glmnet_model,X_test, ptype='response',s='lambda.min')\n",
    "    \n",
    "\n",
    "    fitted_Y_test = fitted_Y_test.flatten()\n",
    "    \n",
    "    assert fitted_Y_test.shape == (X_test.shape[0],), 'fitted_Y should not be two dimensional (hint: reshaping may be helpful)'\n",
    "    assert isinstance(glmnet_model, dict)\n",
    "    return fitted_Y_test, glmnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6db6d730c4d9bd621e840bbb2f726f25",
     "grade": true,
     "grade_id": "cell-a068d28a595d2355",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(350).reshape(70,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "some_pred, some_model = glmnet_ridge(some_X, some_Y)\n",
    "assert np.array_equal(some_pred.round(3)[:5], np.array([21.206, 45.052, 40.206, 73.639, 21.206]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(lambda *args,**kwargs: glmnet_ridge(*args,**kwargs)[0], task_id=6)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    train_and_plot(glmnet_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 7</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `glmnet_lasso` that fits a Lasso-regression model from the glmnet library, and takes the following arguments as input:\n",
    "\n",
    "1. `X_train`: A numpy array of the shape `(N,d)` where `N` is the number of training data points, and `d` is the data dimension. Do not assume anything about `N` or `d` other than being a positive integer.\n",
    "2. `Y_train`: A numpy array of the shape `(N,)` where `N` is the number of training data points.\n",
    "3. `X_test`: A numpy array of the shape `(N_test,d)` where `N_test` is the number of testing data points, and `d` is the data dimension.\n",
    "\n",
    "Your model should train on the training features and labels, and then predict on the test data. Your model should return the following two items:\n",
    "\n",
    "1. `fitted_Y_test`: The predicted values on the test data as a numpy array with a shape of `(N_test,)` where `N_test` is the number of testing data points.\n",
    "\n",
    "2. `glmnet_model`: The glmnet library's returned model stored as a python dictionary.\n",
    "\n",
    "**Important Notes**:\n",
    "1. **Do not** play with the default options unless you're instructed to.\n",
    "2. You may find this glmnet documentation helpful: https://github.com/bbalasub1/glmnet_python/blob/master/test/glmnet_examples.ipynb\n",
    "  * You may find it useful to read about the gaussian family in the first section, cross-validation, the functions `cvglmnet` and `cvglmnetPredict`, and their arguments (specially the alpha parameter for `cvglmnet`).\n",
    "3. You **should** perform **cross-validation** for this task.\n",
    "4. Use **10-folds** for cross-validation.\n",
    "5. Ask glmnet to search over **100** different values of the regularization coefficient.\n",
    "6. Use the **Mean Squared Error** as a metric for cross-validation.\n",
    "7. For **prediction**, use the **regularization coefficient** that produces the **minimum cross-validation MSE**.\n",
    "7. You may need to choose the proper `family` variable when you're training the model.\n",
    "8. You may need to choose the proper `ptype` variable when you're predicting on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bd0b44c3cfb8e4357f3b4c75e15bc7d",
     "grade": false,
     "grade_id": "cell-5c462a62425bb5e2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glmnet_lasso(X_train, Y_train, X_test=None):\n",
    "    \n",
    "    if X_test is None:\n",
    "        X_test = X_train.copy().astype(np.float64)\n",
    "    X_train = X_train.copy().astype(np.float64)\n",
    "    Y_train = Y_train.copy().astype(np.float64)\n",
    "    \n",
    "    glmnet_model = cvglmnet(x=X_train,y=Y_train, family='gaussian',alpha=1, folds=10,nlambda=100, ptype='mse')\n",
    "    ##Alpha = 1 para Lasso y según yo es el mismo código\n",
    "    fitted_Y_test = cvglmnetPredict(glmnet_model,X_test, ptype='response',s='lambda.min')\n",
    "    \n",
    "\n",
    "    fitted_Y_test = fitted_Y_test.flatten()\n",
    "    \n",
    "    assert fitted_Y_test.shape == (X_test.shape[0],), 'fitted_Y should not be two dimensional (hint: reshaping may be helpful)'\n",
    "    assert isinstance(glmnet_model, dict)\n",
    "    return fitted_Y_test, glmnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af308ec314cfccef3facc323cab5cd6e",
     "grade": true,
     "grade_id": "cell-3facdc3840e0c79d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(350).reshape(70,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "some_pred, some_model = glmnet_lasso(some_X, some_Y)\n",
    "assert np.array_equal(some_pred.round(3)[:5], np.array([20.716, 45.019, 40.11 , 74.153, 20.716]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(lambda *args,**kwargs: glmnet_lasso(*args,**kwargs)[0], task_id=7)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    train_and_plot(glmnet_lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    _, lasso_model = glmnet_lasso(X_train_val, lat_train_val, X_train_val)\n",
    "    _, ridge_model = glmnet_ridge(X_train_val, lat_train_val, X_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    f = plt.figure(figsize=(9,4), dpi=120)\n",
    "    f.add_subplot(1,2,1)\n",
    "    cvglmnetPlot(lasso_model)\n",
    "    plt.gca().set_title('Lasso-Regression Model')\n",
    "    f.add_subplot(1,2,2)\n",
    "    cvglmnetPlot(ridge_model)\n",
    "    _ = plt.gca().set_title('Ridge-Regression Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    lasso_nz_coefs = np.sum(cvglmnetCoef(lasso_model, s = 'lambda_min') != 0)\n",
    "    ridge_nz_coefs = np.sum(cvglmnetCoef(ridge_model, s = 'lambda_min') != 0)\n",
    "    print(f'A Total of {lasso_nz_coefs} Lasso-Regression coefficients were non-zero.')\n",
    "    print(f'A Total of {ridge_nz_coefs} Ridge-Regression coefficients were non-zero.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Elastic-net Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 8</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `glmnet_elastic` that fits an elastic-net model from the glmnet library, and takes the following arguments as input:\n",
    "\n",
    "1. `X_train`: A numpy array of the shape `(N,d)` where `N` is the number of training data points, and `d` is the data dimension. Do not assume anything about `N` or `d` other than being a positive integer.\n",
    "2. `Y_train`: A numpy array of the shape `(N,)` where `N` is the number of training data points.\n",
    "3. `X_test`: A numpy array of the shape `(N_test,d)` where `N_test` is the number of testing data points, and `d` is the data dimension.\n",
    "4. `alpha`: The elastic-net regularization parameter $\\alpha$.\n",
    "\n",
    "Your model should train on the training features and labels, and then predict on the test data. Your model should return the following two items:\n",
    "\n",
    "1. `fitted_Y_test`: The predicted values on the test data as a numpy array with a shape of `(N_test,)` where `N_test` is the number of testing data points.\n",
    "\n",
    "2. `glmnet_model`: The glmnet library's returned model stored as a python dictionary.\n",
    "\n",
    "**Important Notes**:\n",
    "1. **Do not** play with the default options unless you're instructed to.\n",
    "2. You may find this glmnet documentation helpful: https://github.com/bbalasub1/glmnet_python/blob/master/test/glmnet_examples.ipynb\n",
    "  * You may find it useful to read about the gaussian family in the first section, cross-validation, the functions `cvglmnet` and `cvglmnetPredict`, and their arguments (specially the alpha parameter for `cvglmnet`).\n",
    "3. You **should** perform **cross-validation** for this task.\n",
    "4. Use **10-folds** for cross-validation.\n",
    "5. Ask glmnet to search over **100** different values of the regularization coefficient.\n",
    "6. Use the **Mean Squared Error** as a metric for cross-validation.\n",
    "7. For **prediction**, use the **regularization coefficient** that produces the **minimum cross-validation MSE**.\n",
    "7. You may need to choose the proper `family` variable when you're training the model.\n",
    "8. You may need to choose the proper `ptype` variable when you're predicting on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf8936ea8f98e80ffe52a775ad672e63",
     "grade": false,
     "grade_id": "cell-6b524abfee9cceff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glmnet_elastic(X_train, Y_train, X_test=None, alpha=1):\n",
    "    \n",
    "    if X_test is None:\n",
    "        X_test = X_train.copy().astype(np.float64)\n",
    "    # Creating Scratch Variables For glmnet consumption\n",
    "    X_train = X_train.copy().astype(np.float64)\n",
    "    Y_train = Y_train.copy().astype(np.float64)\n",
    "    \n",
    "    glmnet_model = cvglmnet(x=X_train,y=Y_train, family='gaussian',alpha=alpha, folds=10,nlambda=100, ptype='mse')\n",
    "    ##Alpha = 0.5 para Elastic Net y según yo es el mismo código\n",
    "    fitted_Y_test = cvglmnetPredict(glmnet_model,X_test, ptype='response',s='lambda.min')\n",
    "    \n",
    "\n",
    "    fitted_Y_test = fitted_Y_test.flatten()\n",
    "        \n",
    "    assert fitted_Y_test.shape == (X_test.shape[0],), 'fitted_Y should not be two dimensional (hint: reshaping may be helpful)'\n",
    "    assert isinstance(glmnet_model, dict)\n",
    "    return fitted_Y_test, glmnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "235023895f90f039a114a367c59cb336",
     "grade": true,
     "grade_id": "cell-8aa0f24d2cbfb3dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(350).reshape(70,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)\n",
    "some_pred, some_model = glmnet_elastic(some_X, some_Y, alpha=0.3)\n",
    "assert np.array_equal(some_pred.round(3)[:5], np.array([20.77 , 45.028, 40.125, 74.112, 20.77 ]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(lambda *args,**kwargs: glmnet_elastic(*args,**kwargs)[0], task_id=8)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    alpha = 0.25\n",
    "    train_and_plot(lambda *args, **kwargs: glmnet_elastic(*args, **kwargs, alpha=alpha))\n",
    "    _ = plt.gcf().suptitle(f'alpha = {alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    alpha = 0.5\n",
    "    train_and_plot(lambda *args, **kwargs: glmnet_elastic(*args, **kwargs, alpha=alpha))\n",
    "    _ = plt.gcf().suptitle(f'alpha = {alpha}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    alpha = 0.75\n",
    "    train_and_plot(lambda *args, **kwargs: glmnet_elastic(*args, **kwargs, alpha=alpha))\n",
    "    _ = plt.gcf().suptitle(f'alpha = {alpha}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    _, alpha1_model = glmnet_elastic(X_train_val, lat_train_val, X_train_val, alpha=0.25)\n",
    "    _, alpha2_model = glmnet_elastic(X_train_val, lat_train_val, X_train_val, alpha=0.5)\n",
    "    _, alpha3_model = glmnet_elastic(X_train_val, lat_train_val, X_train_val, alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    f = plt.figure(figsize=(9,3), dpi=120)\n",
    "    f.add_subplot(1,3,1)\n",
    "    cvglmnetPlot(alpha1_model)\n",
    "    plt.gca().set_title(f'Elastic Net (Alpha=0.25)')\n",
    "    f.add_subplot(1,3,2)\n",
    "    cvglmnetPlot(alpha2_model)\n",
    "    plt.gca().set_title(f'Elastic Net (Alpha=0.5)')\n",
    "    f.add_subplot(1,3,3)\n",
    "    cvglmnetPlot(alpha3_model)\n",
    "    _ = plt.gca().set_title(f'Elastic Net (Alpha=0.75)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    alpha1_nz_coefs = np.sum(cvglmnetCoef(alpha1_model, s = 'lambda_min') != 0)\n",
    "    alpha2_nz_coefs = np.sum(cvglmnetCoef(alpha2_model, s = 'lambda_min') != 0)\n",
    "    alpha3_nz_coefs = np.sum(cvglmnetCoef(alpha3_model, s = 'lambda_min') != 0)\n",
    "\n",
    "    print(f'With an alpha of 0.25, a Total of {alpha1_nz_coefs} elastic-net coefficients were non-zero.')\n",
    "    print(f'With an alpha of 0.50, a Total of {alpha2_nz_coefs} elastic-net coefficients were non-zero.')\n",
    "    print(f'With an alpha of 0.75, a Total of {alpha3_nz_coefs} elastic-net coefficients were non-zero.')\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(8,5), dpi=100)\n",
    "    ax.plot([0,0.25,0.5,0.75,1], [ridge_nz_coefs, alpha1_nz_coefs, alpha2_nz_coefs, alpha3_nz_coefs, lasso_nz_coefs])\n",
    "    ax.set_xlabel('The Elastic-Net Alpha Parameter')\n",
    "    ax.set_ylabel('The Number of Non-zero Coefficients')\n",
    "    _ = ax.set_title('The Number of Used Features Vs. The Elastic-Net Alpha Parameter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Machine Learning dataset repository hosts a dataset giving whether a Taiwanese credit card user defaults against a variety of features at http://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Input/Output**: This data has 24 columns; the first 23 columns are the features, and the last column is an indicator variable telling whether the next month's payment was defaulted.\n",
    "\n",
    "* **Missing Data**: There is no missing data.\n",
    "\n",
    "* **Final Goal**: We want to **properly** fit a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('credit/credit.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = df.iloc[:,:-1].values\n",
    "Y_full = df.iloc[:,-1].values\n",
    "X_full.shape, Y_full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_detector = 'LOF'\n",
    "\n",
    "if outlier_detector == 'LOF':\n",
    "    outlier_clf = LocalOutlierFactor(novelty=False)\n",
    "elif outlier_detector == 'IF':\n",
    "    outlier_clf = IsolationForest(warm_start=True, random_state=12345)\n",
    "elif outlier_detector == 'EE':\n",
    "    outlier_clf = EllipticEnvelope(random_state=12345)\n",
    "else:\n",
    "    outlier_clf = None\n",
    "\n",
    "is_not_outlier = outlier_clf.fit_predict(X_full) if outlier_clf is not None else np.ones_like(lat_full)>0\n",
    "X_useful = X_full[is_not_outlier==1,:]\n",
    "Y_useful = Y_full[is_not_outlier==1]\n",
    "\n",
    "X_useful.shape, Y_useful.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train-Validation-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_indices, test_indices = train_test_split(np.arange(X_useful.shape[0]), test_size=0.2, random_state=12345)\n",
    "\n",
    "X_train_val = X_useful[train_val_indices, :]\n",
    "Y_train_val = Y_useful[train_val_indices]\n",
    "\n",
    "X_test = X_useful[test_indices, :]\n",
    "Y_test = Y_useful[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Elastic Net Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Task 9</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `glmnet_logistic_elastic` that fits an elastic-net logistic regression model from the glmnet library, and takes the following arguments as input:\n",
    "\n",
    "1. `X_train`: A numpy array of the shape `(N,d)` where `N` is the number of training data points, and `d` is the data dimension. Do not assume anything about `N` or `d` other than being a positive integer.\n",
    "2. `Y_train`: A numpy array of the shape `(N,)` where `N` is the number of training data points.\n",
    "3. `X_test`: A numpy array of the shape `(N_test,d)` where `N_test` is the number of testing data points, and `d` is the data dimension.\n",
    "4. `alpha`: The elastic-net regularization parameter $\\alpha$.\n",
    "\n",
    "Your model should train on the training features and labels, and then predict on the test data. Your model should return the following two items:\n",
    "\n",
    "1. `fitted_Y_test`: The predicted values on the test data as a numpy array with a shape of `(N_test,)` where `N_test` is the number of testing data points. These values should indicate the prediction classes for test data, and should be either 0 or 1.\n",
    "\n",
    "2. `glmnet_model`: The glmnet library's returned model stored as a python dictionary.\n",
    "\n",
    "**Important Notes**:\n",
    "1. **Do not** play with the default options unless you're instructed to.\n",
    "2. You may find this glmnet documentation helpful: https://github.com/bbalasub1/glmnet_python/blob/master/test/glmnet_examples.ipynb\n",
    "  * You may find it useful to read about the logistic family in the last sections.\n",
    "3. You **should** perform **cross-validation** for this task.\n",
    "4. Use **10-folds** for cross-validation.\n",
    "5. Ask glmnet to search over **100** different values of the regularization coefficient.\n",
    "6. Use the **Misclassification Error** as a metric for cross-validation.\n",
    "7. For **prediction**, use the **regularization coefficient** that produces the **minimum cross-validation misclassification**.\n",
    "7. You may need to choose the proper `family` variable when you're training the model.\n",
    "8. You may need to choose the proper `ptype` variable when you're predicting on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f0442e1628d97ac79e50a4583c7267a8",
     "grade": false,
     "grade_id": "cell-116057c7411df8de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def glmnet_logistic_elastic(X_train, Y_train, X_test=None, alpha=1):\n",
    "\n",
    "    if X_test is None:\n",
    "        X_test = X_train.copy().astype(np.float64)\n",
    "    \n",
    "    # Creating Scratch Variables For glmnet Consumption\n",
    "    X_train = X_train.copy().astype(np.float64)\n",
    "    Y_train = Y_train.copy().astype(np.float64)\n",
    "    \n",
    "    # Set up the parameters for Elastic Net logistic regression\n",
    "    # family='binomial' for logistic regression\n",
    "    # type.measure='class' for misclassification error\n",
    "    glmnet_model = cvglmnet(\n",
    "        x=X_train,\n",
    "        y=Y_train,\n",
    "        family='binomial',  \n",
    "        nfolds=10,  \n",
    "        alpha=alpha,      \n",
    "        nlambda=100,       \n",
    "        maxit=1000000,    \n",
    "        type_measure='class' \n",
    "    )\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    # Use lambda.min to use the lambda that gives minimum misclassification error\n",
    "    # ptype='class' to get the predicted classes (0 or 1)\n",
    "    fitted_Y_test = cvglmnetPredict(\n",
    "        glmnet_model,\n",
    "        X_test,\n",
    "        ptype='class',    # Get class predictions\n",
    "        s='lambda.min'    # Use lambda that gives minimum misclassification\n",
    "    )\n",
    "    \n",
    "\n",
    "    fitted_Y_test = fitted_Y_test.flatten()\n",
    "    \n",
    "   \n",
    "    assert fitted_Y_test.shape == (X_test.shape[0],), 'fitted_Y should not be two dimensional (hint: reshaping may be helpful)'\n",
    "    assert isinstance(glmnet_model, dict)\n",
    "    return fitted_Y_test, glmnet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96d562473bb512ee1353db6103e839ac",
     "grade": true,
     "grade_id": "cell-07eefe84ea29503c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "some_X = (np.arange(350).reshape(70,5) ** 13) % 20\n",
    "some_Y = np.sum(some_X, axis=1)%2\n",
    "some_pred, some_model = glmnet_logistic_elastic(some_X, some_Y, alpha=0.3)\n",
    "assert np.array_equal(some_pred.round(3)[:5], np.array([0., 0., 0., 1., 0.]))\n",
    "\n",
    "# Checking against the pre-computed test database\n",
    "test_results = test_case_checker(lambda *args,**kwargs: glmnet_logistic_elastic(*args,**kwargs)[0], task_id=9)\n",
    "assert test_results['passed'], test_results['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    _, ridge_model = glmnet_logistic_elastic(X_train_val, Y_train_val, X_train_val, alpha=0.00)\n",
    "    _, alpha1_model = glmnet_logistic_elastic(X_train_val, Y_train_val, X_train_val, alpha=0.25)\n",
    "    _, alpha2_model = glmnet_logistic_elastic(X_train_val, Y_train_val, X_train_val, alpha=0.50)\n",
    "    _, alpha3_model = glmnet_logistic_elastic(X_train_val, Y_train_val, X_train_val, alpha=0.75)\n",
    "    _, lasso_model = glmnet_logistic_elastic(X_train_val, Y_train_val, X_train_val, alpha=1.00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    f = plt.figure(figsize=(9,4), dpi=120)\n",
    "    f.add_subplot(1,2,1)\n",
    "    cvglmnetPlot(lasso_model)\n",
    "    plt.gca().set_title('Lasso-Regression Model')\n",
    "    f.add_subplot(1,2,2)\n",
    "    cvglmnetPlot(ridge_model)\n",
    "    _ = plt.gca().set_title('Ridge-Regression Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    f = plt.figure(figsize=(9,3), dpi=120)\n",
    "    f.add_subplot(1,3,1)\n",
    "    cvglmnetPlot(alpha1_model)\n",
    "    plt.gca().set_title(f'Elastic Net (Alpha=0.25)')\n",
    "    f.add_subplot(1,3,2)\n",
    "    cvglmnetPlot(alpha2_model)\n",
    "    plt.gca().set_title(f'Elastic Net (Alpha=0.5)')\n",
    "    f.add_subplot(1,3,3)\n",
    "    cvglmnetPlot(alpha3_model)\n",
    "    _ = plt.gca().set_title(f'Elastic Net (Alpha=0.75)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_computation:\n",
    "    lasso_nz_coefs = np.sum(cvglmnetCoef(lasso_model, s = 'lambda_min') != 0)\n",
    "    ridge_nz_coefs = np.sum(cvglmnetCoef(ridge_model, s = 'lambda_min') != 0)\n",
    "    alpha1_nz_coefs = np.sum(cvglmnetCoef(alpha1_model, s = 'lambda_min') != 0)\n",
    "    alpha2_nz_coefs = np.sum(cvglmnetCoef(alpha2_model, s = 'lambda_min') != 0)\n",
    "    alpha3_nz_coefs = np.sum(cvglmnetCoef(alpha3_model, s = 'lambda_min') != 0)\n",
    "\n",
    "    print(f'A Total of {ridge_nz_coefs} Ridge-Regression coefficients were non-zero.')\n",
    "    print(f'With an alpha of 0.25, a Total of {alpha1_nz_coefs} elastic-net coefficients were non-zero.')\n",
    "    print(f'With an alpha of 0.50, a Total of {alpha2_nz_coefs} elastic-net coefficients were non-zero.')\n",
    "    print(f'With an alpha of 0.75, a Total of {alpha3_nz_coefs} elastic-net coefficients were non-zero.')\n",
    "    print(f'A Total of {lasso_nz_coefs} Lasso-Regression coefficients were non-zero.')\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(8,5), dpi=100)\n",
    "    ax.plot([0,0.25,0.5,0.75,1], [ridge_nz_coefs, alpha1_nz_coefs, alpha2_nz_coefs, alpha3_nz_coefs, lasso_nz_coefs])\n",
    "    ax.set_xlabel('The Elastic-Net Alpha Parameter')\n",
    "    ax.set_ylabel('The Number of Non-zero Coefficients')\n",
    "    _ = ax.set_title('The Number of Used Features Vs. The Elastic-Net Alpha Parameter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
